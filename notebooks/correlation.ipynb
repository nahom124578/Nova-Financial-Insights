{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Alignmnent \n",
    "## normalize dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date      Open      High       Low     Close  \\\n",
      "0 1980-12-12 00:00:00+00:00  0.128348  0.128906  0.128348  0.128348   \n",
      "1 1980-12-15 00:00:00+00:00  0.122210  0.122210  0.121652  0.121652   \n",
      "2 1980-12-16 00:00:00+00:00  0.113281  0.113281  0.112723  0.112723   \n",
      "3 1980-12-17 00:00:00+00:00  0.115513  0.116071  0.115513  0.115513   \n",
      "4 1980-12-18 00:00:00+00:00  0.118862  0.119420  0.118862  0.118862   \n",
      "\n",
      "   Adj Close     Volume  Dividends  Stock Splits  Unnamed: 0  \\\n",
      "0   0.098943  469033600        0.0           0.0      357064   \n",
      "1   0.093781  175884800        0.0           0.0      357064   \n",
      "2   0.086898  105728000        0.0           0.0      357064   \n",
      "3   0.089049   86441600        0.0           0.0      357064   \n",
      "4   0.091630   73449600        0.0           0.0      357064   \n",
      "\n",
      "                                            headline  \\\n",
      "0  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "1  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "2  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "3  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "4  ETFs To Watch April 28, 2011 (DGP, IEO, PRN, IDX)   \n",
      "\n",
      "                                                 url      publisher  \\\n",
      "0  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "1  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "2  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "3  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "4  https://www.benzinga.com/etfs/bond-etfs/11/04/...  ETF Professor   \n",
      "\n",
      "                       date stock  \n",
      "0 2011-04-28 01:01:48+00:00   DGP  \n",
      "1 2011-04-28 01:01:48+00:00   DGP  \n",
      "2 2011-04-28 01:01:48+00:00   DGP  \n",
      "3 2011-04-28 01:01:48+00:00   DGP  \n",
      "4 2011-04-28 01:01:48+00:00   DGP  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load news dataset (analyst ratings)\n",
    "news_df = pd.read_csv('../assets/data/raw_analyst_ratings.csv')\n",
    "\n",
    "# Load stock price datasets for each stock\n",
    "stock_dfs = {\n",
    "    'AAPL': pd.read_csv('../assets/data/AAPL_historical_data.csv'),\n",
    "    'AMZN': pd.read_csv('../assets/data/AMZN_historical_data.csv'),\n",
    "    'GOOG': pd.read_csv('../assets/data/GOOG_historical_data.csv'),\n",
    "    'MSFT': pd.read_csv('../assets/data/MSFT_historical_data.csv'),\n",
    "    'META': pd.read_csv('../assets/data/META_historical_data.csv'),\n",
    "    'NVDA': pd.read_csv('../assets/data/NVDA_historical_data.csv'),\n",
    "    'TSLA': pd.read_csv('../assets/data/TSLA_historical_data.csv')\n",
    "}\n",
    "\n",
    "# Normalize timestamps for news data (ensure both date and time are captured)\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaT in the 'date' column\n",
    "news_df = news_df.dropna(subset=['date'])\n",
    "\n",
    "# Normalize timestamps for stock price data (ensure both date and time are captured)\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'], errors='coerce')\n",
    "\n",
    "# Convert stock Date columns to timezone-aware (UTC) to match news data\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    if stock_data['Date'].dt.tz is None:  # Check if timezone is naive\n",
    "        stock_data['Date'] = stock_data['Date'].dt.tz_localize('UTC')  # Localize to UTC\n",
    "    else:\n",
    "        stock_data['Date'] = stock_data['Date'].dt.tz_convert('UTC')  # Convert to UTC if already aware\n",
    "\n",
    "# Convert news date column to UTC timezone (if it's not already)\n",
    "if news_df['date'].dt.tz is None:  # Check if timezone is naive\n",
    "    news_df['date'] = news_df['date'].dt.tz_localize('UTC')  # Localize to UTC\n",
    "else:\n",
    "    news_df['date'] = news_df['date'].dt.tz_convert('UTC')  # Convert to UTC if already aware\n",
    "\n",
    "# Align news with stock prices\n",
    "aligned_dfs = {}\n",
    "\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    # Merge the stock data with news data based on the nearest date\n",
    "    aligned_news = pd.merge_asof(\n",
    "        stock_data.sort_values('Date'),\n",
    "        news_df.sort_values('date'),\n",
    "        left_on='Date',\n",
    "        right_on='date',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    aligned_dfs[stock] = aligned_news\n",
    "\n",
    "# Handle missing data by dropping rows with missing values after the merge\n",
    "for stock in aligned_dfs:\n",
    "    aligned_dfs[stock] = aligned_dfs[stock].dropna()\n",
    "\n",
    "# Example: Check the first few rows of the aligned data for AAPL\n",
    "aapl_aligned_data = aligned_dfs['AAPL']\n",
    "print(aapl_aligned_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "sentiment\n",
      "Neutral     934914\n",
      "Positive    341178\n",
      "Negative    131236\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Load data from a CSV file\n",
    "file_path = \"../assets/data/raw_analyst_ratings.csv\"  # Replace with the actual path to your file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the dataset has a 'headline' column\n",
    "if 'headline' not in df.columns:\n",
    "    raise ValueError(\"The dataset must contain a 'headline' column.\")\n",
    "\n",
    "# Perform sentiment analysis\n",
    "def analyze_sentiment(headline):\n",
    "    analysis = TextBlob(headline)\n",
    "    polarity = analysis.polarity  # Sentiment polarity: -1 (negative) to +1 (positive)\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Apply the sentiment analysis function to the headlines\n",
    "df['sentiment'] = df['headline'].apply(analyze_sentiment)\n",
    "\n",
    "# Count the number of headlines by sentiment\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(sentiment_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Stock Returns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Close  Daily_Return\n",
      "Date                              \n",
      "1980-12-12  0.128348           NaN\n",
      "1980-12-15  0.121652     -5.217061\n",
      "1980-12-16  0.112723     -7.339788\n",
      "1980-12-17  0.115513      2.475091\n",
      "1980-12-18  0.118862      2.899246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of stock filenames\n",
    "stock_files = {\n",
    "    'AAPL': '../assets/data/AAPL_historical_data.csv',\n",
    "    'AMZN': '../assets/data/AMZN_historical_data.csv',\n",
    "    'GOOG': '../assets/data/GOOG_historical_data.csv',\n",
    "    'MSFT': '../assets/data/MSFT_historical_data.csv',\n",
    "    'META': '../assets/data/META_historical_data.csv',\n",
    "    'NVDA': '../assets/data/NVDA_historical_data.csv',\n",
    "    'TSLA': '../assets/data/TSLA_historical_data.csv'\n",
    "}\n",
    "\n",
    "# Dictionary to store daily returns for each stock\n",
    "daily_returns = {}\n",
    "\n",
    "# Loop through each stock and calculate the daily returns\n",
    "for stock, file_path in stock_files.items():\n",
    "    # Load stock data\n",
    "    stock_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the 'Date' column is in datetime format\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "    # Set 'Date' as the index\n",
    "    stock_data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Calculate daily returns based on the 'Close' price (percentage change)\n",
    "    stock_data['Daily_Return'] = stock_data['Close'].pct_change() * 100\n",
    "\n",
    "    # Store the daily returns in the dictionary\n",
    "    daily_returns[stock] = stock_data[['Close', 'Daily_Return']]\n",
    "\n",
    "    # Optionally, save the results to a new CSV file\n",
    "    stock_data.to_csv(f'../assets/data/{stock}_with_daily_returns.csv')\n",
    "\n",
    "# Example: Print the first few rows of the daily returns for AAPL\n",
    "print(daily_returns['AAPL'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'headline', 'url', 'publisher', 'date', 'stock'], dtype='object')\n",
      "                       date  sentiment_score\n",
      "0 2011-04-28 01:01:48+00:00         0.000000\n",
      "1 2011-04-28 17:49:29+00:00         0.136364\n",
      "2 2011-04-28 19:00:36+00:00         0.000000\n",
      "3 2011-04-29 17:47:06+00:00        -0.166667\n",
      "4 2011-04-29 20:11:05+00:00         0.500000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample news data with 'date' and 'sentiment_score' columns\n",
    "news_data = pd.read_csv('../assets/data/raw_analyst_ratings.csv')\n",
    "\n",
    "# Ensure the 'date' column is in datetime format\n",
    "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
    "\n",
    "print(news_data.columns) \n",
    "\n",
    "# Function to calculate sentiment polarity using TextBlob\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(str(text))  # Ensure text is in string format\n",
    "    return blob.sentiment.polarity  # You can also use blob.sentiment.subjectivity if needed\n",
    "\n",
    "# Calculate sentiment scores for each article\n",
    "news_data['sentiment_score'] = news_data['headline'].apply(get_sentiment)\n",
    "\n",
    "# Check if timezone is naive and localize or convert to UTC\n",
    "if news_data['date'].dt.tz is None:  # Check if timezone is naive\n",
    "    news_data['date'] = news_data['date'].dt.tz_localize('UTC')  # Localize to UTC\n",
    "else:\n",
    "    news_data['date'] = news_data['date'].dt.tz_convert('UTC')\n",
    "\n",
    "# Calculate daily average sentiment scores by grouping by 'date'\n",
    "daily_sentiment = news_data.groupby('date')['sentiment_score'].mean().reset_index()\n",
    "\n",
    "# Display the first few rows of the aggregated sentiment scores\n",
    "print(daily_sentiment.head())\n",
    "\n",
    "# Optionally, save the aggregated data to a CSV file\n",
    "daily_sentiment.to_csv('../assets/data/daily_sentiment_scores.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalized data between daily return and sentiment score for easily computation of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged data for AAPL to ../assets/data/AAPL_merged.csv\n",
      "Saved merged data for AMZN to ../assets/data/AMZN_merged.csv\n",
      "Saved merged data for GOOG to ../assets/data/GOOG_merged.csv\n",
      "Saved merged data for MSFT to ../assets/data/MSFT_merged.csv\n",
      "Saved merged data for META to ../assets/data/META_merged.csv\n",
      "Saved merged data for NVDA to ../assets/data/NVDA_merged.csv\n",
      "Saved merged data for TSLA to ../assets/data/TSLA_merged.csv\n",
      "                       Date      Open      High       Low     Close  \\\n",
      "1 1980-12-15 00:00:00+00:00  0.122210  0.122210  0.121652  0.121652   \n",
      "2 1980-12-16 00:00:00+00:00  0.113281  0.113281  0.112723  0.112723   \n",
      "3 1980-12-17 00:00:00+00:00  0.115513  0.116071  0.115513  0.115513   \n",
      "4 1980-12-18 00:00:00+00:00  0.118862  0.119420  0.118862  0.118862   \n",
      "5 1980-12-19 00:00:00+00:00  0.126116  0.126674  0.126116  0.126116   \n",
      "\n",
      "   Adj Close     Volume  Dividends  Stock Splits  Daily_Return  \\\n",
      "1   0.093781  175884800        0.0           0.0     -5.217061   \n",
      "2   0.086898  105728000        0.0           0.0     -7.339788   \n",
      "3   0.089049   86441600        0.0           0.0      2.475091   \n",
      "4   0.091630   73449600        0.0           0.0      2.899246   \n",
      "5   0.097223   48630400        0.0           0.0      6.102867   \n",
      "\n",
      "                       date  sentiment_score  \n",
      "1 2011-04-28 01:01:48+00:00              0.0  \n",
      "2 2011-04-28 01:01:48+00:00              0.0  \n",
      "3 2011-04-28 01:01:48+00:00              0.0  \n",
      "4 2011-04-28 01:01:48+00:00              0.0  \n",
      "5 2011-04-28 01:01:48+00:00              0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load news dataset (analyst ratings)\n",
    "news_df = pd.read_csv('../assets/data/daily_sentiment_scores.csv')\n",
    "\n",
    "# Load stock price datasets for each stock\n",
    "stock_dfs = {\n",
    "    'AAPL': pd.read_csv('../assets/data/AAPL_with_daily_returns.csv'),\n",
    "    'AMZN': pd.read_csv('../assets/data/AMZN_with_daily_returns.csv'),\n",
    "    'GOOG': pd.read_csv('../assets/data/GOOG_with_daily_returns.csv'),\n",
    "    'MSFT': pd.read_csv('../assets/data/MSFT_with_daily_returns.csv'),\n",
    "    'META': pd.read_csv('../assets/data/META_with_daily_returns.csv'),\n",
    "    'NVDA': pd.read_csv('../assets/data/NVDA_with_daily_returns.csv'),\n",
    "    'TSLA': pd.read_csv('../assets/data/TSLA_with_daily_returns.csv')\n",
    "}\n",
    "\n",
    "# Normalize timestamps for news data (ensure both date and time are captured)\n",
    "news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaT in the 'date' column\n",
    "news_df = news_df.dropna(subset=['date'])\n",
    "\n",
    "# Normalize timestamps for stock price data (ensure both date and time are captured)\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'], errors='coerce')\n",
    "\n",
    "# Convert stock Date columns to timezone-aware (UTC) to match news data\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    if stock_data['Date'].dt.tz is None:  # Check if timezone is naive\n",
    "        stock_data['Date'] = stock_data['Date'].dt.tz_localize('UTC')  # Localize to UTC\n",
    "    else:\n",
    "        stock_data['Date'] = stock_data['Date'].dt.tz_convert('UTC')  # Convert to UTC if already aware\n",
    "\n",
    "# Convert news date column to UTC timezone (if it's not already)\n",
    "if news_df['date'].dt.tz is None:  # Check if timezone is naive\n",
    "    news_df['date'] = news_df['date'].dt.tz_localize('UTC')  # Localize to UTC\n",
    "else:\n",
    "    news_df['date'] = news_df['date'].dt.tz_convert('UTC')  # Convert to UTC if already aware\n",
    "\n",
    "# Align news with stock prices\n",
    "aligned_dfs = {}\n",
    "\n",
    "for stock, stock_data in stock_dfs.items():\n",
    "    # Merge the stock data with news data based on the nearest date\n",
    "    aligned_news = pd.merge_asof(\n",
    "        stock_data.sort_values('Date'),\n",
    "        news_df.sort_values('date'),\n",
    "        left_on='Date',\n",
    "        right_on='date',\n",
    "        direction='nearest'\n",
    "    )\n",
    "    aligned_dfs[stock] = aligned_news\n",
    "\n",
    "# Handle missing data by dropping rows with missing values after the merge\n",
    "for stock in aligned_dfs:\n",
    "    aligned_dfs[stock] = aligned_dfs[stock].dropna()\n",
    "\n",
    "    # Optionally, you can save the aligned data to CSV files\n",
    "\n",
    "for stock, aligned_data in aligned_dfs.items():\n",
    "    output_file = f'../assets/data/{stock}_merged.csv'\n",
    "    aligned_data.to_csv(output_file, index=False)\n",
    "    print(f\"Saved merged data for {stock} to {output_file}\")\n",
    "# Example: Check the first few rows of the aligned data for AAPL\n",
    "aapl_aligned_data = aligned_dfs['AAPL']\n",
    "print(aapl_aligned_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows after dropping missing data in ../assets/data/NVDA_merged.csv: 6420\n",
      "Pearson Correlation Coefficient for NVDA_merged.csv: -0.0031275917625069255\n",
      "Total rows after dropping missing data in ../assets/data/GOOG_merged.csv: 5019\n",
      "Pearson Correlation Coefficient for GOOG_merged.csv: 0.0014329625630151168\n",
      "Total rows after dropping missing data in ../assets/data/AAPL_merged.csv: 10997\n",
      "Pearson Correlation Coefficient for AAPL_merged.csv: -0.005824893410032895\n",
      "Total rows after dropping missing data in ../assets/data/AMZN_merged.csv: 6845\n",
      "Pearson Correlation Coefficient for AMZN_merged.csv: -0.002207884687214358\n",
      "Total rows after dropping missing data in ../assets/data/META_merged.csv: 2925\n",
      "Pearson Correlation Coefficient for META_merged.csv: -0.0022751516495962873\n",
      "Total rows after dropping missing data in ../assets/data/MSFT_merged.csv: 9671\n",
      "Pearson Correlation Coefficient for MSFT_merged.csv: -0.004191274606370307\n",
      "Total rows after dropping missing data in ../assets/data/TSLA_merged.csv: 3544\n",
      "Pearson Correlation Coefficient for TSLA_merged.csv: -0.0061460251324803265\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def calculate_pearson_correlation_from_merged_file(merged_file_path):\n",
    "    \"\"\"\n",
    "    Calculates the Pearson correlation coefficient between daily sentiment scores \n",
    "    and stock daily returns from a single merged file.\n",
    "\n",
    "    Parameters:\n",
    "        merged_file_path (str): Path to the merged CSV file.\n",
    "\n",
    "    Returns:\n",
    "        float: Pearson correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Load the merged data\n",
    "    merged_df = pd.read_csv(merged_file_path)\n",
    "\n",
    "    # Standardize column names\n",
    "    merged_df.columns = merged_df.columns.str.strip().str.lower()\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if not {'sentiment_score', 'daily_return'}.issubset(merged_df.columns):\n",
    "        raise ValueError(f\"File {merged_file_path} must have 'sentiment_score' and 'daily_return' columns.\")\n",
    "    \n",
    "    # Drop rows with missing data\n",
    "    merged_df = merged_df.dropna(subset=['sentiment_score', 'daily_return'])\n",
    "\n",
    "    # Debugging: Check the number of valid rows\n",
    "    print(f\"Total rows after dropping missing data in {merged_file_path}: {len(merged_df)}\")\n",
    "\n",
    "    # Ensure sufficient data points\n",
    "    if len(merged_df) < 2:\n",
    "        raise ValueError(f\"Not enough data points for correlation calculation in {merged_file_path}.\")\n",
    "    \n",
    "    # Extract data for correlation\n",
    "    sentiment_scores = merged_df['sentiment_score'].tolist()\n",
    "    stock_returns = merged_df['daily_return'].tolist()\n",
    "\n",
    "    # Calculate Pearson correlation coefficient\n",
    "    correlation = np.corrcoef(sentiment_scores, stock_returns)[0, 1]\n",
    "\n",
    "    return correlation\n",
    "\n",
    "# Path to the directory where the merged stock files are stored\n",
    "merged_data_directory = '../assets/data/'\n",
    "\n",
    "# List all CSV files in the directory that start with the stock name and end with '_merged.csv'\n",
    "merged_files = [f for f in os.listdir(merged_data_directory) if f.endswith('_merged.csv')]\n",
    "\n",
    "# Calculate Pearson correlation for each stock\n",
    "for merged_file in merged_files:\n",
    "    merged_file_path = os.path.join(merged_data_directory, merged_file)\n",
    "    \n",
    "    try:\n",
    "        pearson_corr = calculate_pearson_correlation_from_merged_file(merged_file_path)\n",
    "        print(f\"Pearson Correlation Coefficient for {merged_file}: {pearson_corr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {merged_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
